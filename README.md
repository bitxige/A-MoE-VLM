# A-MoE-VLM: Logic-Aligned Compact Vision-Language Models

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![Framework](https://img.shields.io/badge/Framework-ms--swift-orange.svg)](https://github.com/modelscope/swift)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

Official implementation of the **Logic-V** alignment pipeline for highly compact Vision-Language Models. This repository provides the complete training logic, dynamic ORB keyframe extraction tools, and pre-trained LoRA weights to reproduce our state-of-the-art results on complex video reasoning tasks.

Moving beyond the flawed "Distillation-then-RL" paradigm‚Äîwhich we identify as causing "Cognitive Mimicry Collapse"‚Äîour highly compact **3B-parameter** model relies on pure Reinforcement Learning via the Logic-V framework. This approach achieves remarkable reasoning capabilities, significantly closing the gap with massive proprietary models.

## üåü Key Results & Radical Transparency
- **Single Expert (Logic-V RL)**: ~59.0% Accuracy on EgoSchema
*(Note: To emphasize the core contribution of the Logic-V reward mechanism, this repository highlights the single-expert 3B model. Our full A-MoE framework builds upon this to achieve 60.8% accuracy.)*

### ‚ö° 100% Reproducible in Under 3 Hours
We understand the reproducibility crisis in current AI research. To demonstrate absolute confidence in our framework, we have engineered this repository for **rapid, out-of-the-box verification**:
* **Rigorously Tested Consistency:** We have independently run the RL single-expert inference pipeline three times on the same server environment. It consistently achieves **~59.0% accuracy** with minimal variance.
* **Everything is Provided:** You do not need to process raw videos. We provide the fully pre-extracted 72B-INT4 semantic logs/QA pairs (`evaluation/egoschema_72B-Int4_analysis.jsonl`), our converged RL LoRA weights (`checkpoint/`), and the evaluation scripts (`inference/eval_rl_single_pass.py`).
* **Lightning Fast:** Reviewers and researchers can fully verify our 59.0% baseline in **less than 3 hours** on a single 48GB GPU (e.g., RTX 4090 / A800).

### üîç Unprocessed Reasoning Logs (Warts and All)
Skeptical of the 3B model's capabilities? We welcome the scrutiny. Inside the `evaluation/` directory, we explicitly provide our **unprocessed RL test result QA pairs (raw `.jsonl` prediction logs)** containing the complete `<think>` traces generated by our agent. You can directly inspect the model's successful causal deductions, as well as its authentic, flawed artifacts (e.g., reward-hacking formatting drifts like `<think>\n <think>`), which explicitly validates the necessity of the A-MoE routing architecture discussed in our paper.

---

## üìÇ Repository Structure

```text
A-MoE-VLM/
‚îú‚îÄ‚îÄ checkpoint/           # Pre-trained LoRA weights (Download required)
‚îú‚îÄ‚îÄ dataset/              # Subsets and JSON annotations for evaluation
‚îú‚îÄ‚îÄ evaluation/           # Raw prediction logs & 72B-Teacher analysis texts
‚îú‚îÄ‚îÄ inference/            # Scripts for reproducing the 59.0% accuracy
‚îî‚îÄ‚îÄ scripts/              # RL training launcher, Logic-V rewards, and 72B analysis tools
```

---

## üöÄ 1. Quick Start & Installation

Clone this repository and set up the environment. We highly recommend using Python 3.10+ and CUDA 11.8+.

```bash
git clone [https://github.com/bitxige/A-MoE-VLM.git](https://github.com/bitxige/A-MoE-VLM.git)
cd A-MoE-VLM
```

**Core Environment Dependencies:**
- `Python` >= 3.10
- `Transformers` >= 4.40.0
- `ms-swift` (for RLHF and LoRA training)
- `DeepSpeed` >= 0.14.0 (for RL acceleration)
- `qwen-vl-utils` & `opencv-python` (for visual processing)

You can easily install the core dependencies via:
```bash
pip install -r requirements.txt
```

---

## üì• 2. Download Pre-trained Weights

To facilitate immediate reproduction, we fully open-source our 5600-step RL LoRA weights. You do not need to retrain the model from scratch.

Download the `adapter_model.safetensors` and place it in the `checkpoint/checkpoint-5600/` directory:

```bash
mkdir -p checkpoint/checkpoint-5600
# Download the core weights (228MB)
wget [https://github.com/bitxige/A-MoE-VLM/releases/download/v1.0/adapter_model.safetensors](https://github.com/bitxige/A-MoE-VLM/releases/download/v1.0/adapter_model.safetensors) -O checkpoint/checkpoint-5600/adapter_model.safetensors
```

> ‚ö†Ô∏è **Note:** Ensure that the accompanying `adapter_config.json` is also placed in the same directory. If your local folder name differs, please adjust the paths accordingly.

---

## üìä 3. Reproducing Evaluations (Inference)

All evaluation scripts are neatly organized in the `inference/` directory.

> ‚ö†Ô∏è **Path & Filename Notice:** The exact filenames for your scripts or datasets might vary based on your local setup. **Please verify and replace the placeholder paths in the commands below with your actual local file paths before running.**

### Evaluate Single RL Expert (~59.0%)
Runs the single generalist expert trained via the Logic-V reward pipeline.

```bash
python inference/eval_rl_single_pass.py \
    --base_model_path "Qwen/Qwen2.5-VL-3B-Instruct" \
    --model_path "checkpoint/checkpoint-5600" \
    --input_file "evaluation/egoschema_72B-Int4_analysis.jsonl" \
    --output_file "evaluation/RL-egoschema_predictions.jsonl"
```

### Evaluate SFT Baseline
For performance comparison against the standard supervised fine-tuned model:

```bash
python inference/eval_sft.py \
    --base_model_path "Qwen/Qwen2.5-VL-3B-Instruct" \
    --lora_path "checkpoint/Qwen2.5-3B-Video-SFT/checkpoint-504" \
    --input_file "evaluation/egoschema_72B-Int4_analysis.jsonl" \
    --output_file "evaluation/sft_egoschema_predictions.jsonl" \
    --max_new_tokens 1024
```

---

## üß† 4. Teacher Annotations & Logic-V Reward

We provide the exact scripts used to generate the logical reasoning anchors and dynamic visual features from the 72B teacher model.

### Extract Visual Logs & ORB Keyframes (72B-Int4)
This script utilizes an ORB-based homography extraction pipeline to filter redundant frames and uses Qwen2.5-72B to build chronological semantic logs.

```bash
# Example usage for extracting teacher logs
python scripts/analyze_72b.py \
    --model_path "Qwen/Qwen2.5-VL-72B-Instruct-AWQ" \
    --video_dir "dataset/videos" \
    --input_json "dataset/egoschema.json" \
    --output_file "evaluation/egoschema_72B-Int4_analysis.jsonl" \
    --max_frames 24
```

### Logic-V Reward Engineering
The core logic for our RL pipeline (checking logical consistency via external arbiters and formatting) is located in the `scripts/train/` directory (e.g., `rewards.py`).

---

## üõ°Ô∏è Reproducibility Note

The reported accuracy (59.0% for the RL single-expert) was evaluated using `bfloat16` precision on a **single 48GB GPU** (e.g., vGPU-48GB / RTX 4090 / A800), which is highly efficient and more than sufficient for inference. *(Note: During the Reinforcement Learning phase, a dual-GPU setup was utilized solely to accelerate the training process.)*

Please note that due to the inherent non-determinism in parallel CUDA operations, architectural variations in Bfloat16 matrix multiplications across different GPU models, and specific environment configurations (e.g., CUDA 11.8, PyTorch, Transformers versions), **you may observe natural fluctuations in the final reproduced accuracy.** To ensure absolute transparency and academic integrity, we have provided our **exact raw prediction logs** containing the complete `<think>` traces in the `evaluation/` directory, serving as the definitive ground-truth for our reported metrics.

---

## ü§ù Acknowledgments
This code is built upon the excellent [ms-swift](https://github.com/modelscope/swift) framework for LLM/VLM fine-tuning and RLHF.
