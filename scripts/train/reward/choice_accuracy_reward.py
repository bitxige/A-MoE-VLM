# -*- coding: utf-8 -*-
"""
Logic-V Reward Engineering: Choice Accuracy Evaluator
This module defines the deterministic outcome reward function for the A-MoE RL pipeline.
It utilizes regular expressions to robustly extract multiple-choice answers (A-H) 
from the VLM's generated reasoning traces and evaluates them against the ground truth.
"""

import re
import warnings
import importlib.util
from typing import Dict, List, Union, Optional

from swift.plugin.orm import ORM
from swift.plugin.orm import orms  # Directly import the framework's registry

class ChoiceAccuracy(ORM):
    """
    Choice accuracy reward function for Reinforcement Learning.
    
    Extracts the option (A-H) from the model's generated text and compares it 
    with the ground truth. Grants a reward of 1.0 for a match, and 0.0 otherwise.
    """

    # Class constants for robust answer extraction
    BOXED_PATTERN = r"\$\\boxed\{([A-H])\}\$"  # Matches standard LaTeX boxed format
    ANSWER_TAG_PATTERN = r"<answer>\s*([A-H])\s*(?:</answer>)?"  # Matches explicit <answer>A</answer> tags
    SIMPLE_DOT_PATTERN = r"(?:^|[^A-Za-z])([A-H])\s*\."  # Matches "A." pattern
    SIMPLE_PATTERN = r"(?:^|[^A-Za-z])([A-H])(?:$|[^A-Za-z])"  # Matches isolated letters
    
    VALID_OPTIONS = set('ABCDEFGH')

    def __init__(self):
        """
        Initializes the ChoiceAccuracy reward function and validates environment dependencies.
        """
        # Ensure mathematical verification dependencies are present if required by the pipeline
        assert importlib.util.find_spec('math_verify') is not None, (
            "⚠️ The 'math_verify' package is required for RL reward computation but not installed. "
            "Please resolve this by running: pip install math_verify"
        )

    def normalize_answer(self, answer: str) -> str:
        """
        Normalizes the raw text response to extract the final predicted option.

        Args:
            answer (str): The raw text output generated by the VLM.

        Returns:
            str: A single normalized uppercase letter (A-H) representing the choice.
        """
        answer = answer.strip()

        # Priority 1: Standard mathematical boxed format ($\boxed{X}$)
        boxed_matches = list(re.finditer(self.BOXED_PATTERN, answer, re.IGNORECASE))
        if boxed_matches:
            return boxed_matches[-1].group(1).upper()

        # Priority 2: Explicit XML-style answer tags (<answer>X</answer>)
        tag_matches = list(re.finditer(self.ANSWER_TAG_PATTERN, answer, re.IGNORECASE))
        if tag_matches:
            return tag_matches[-1].group(1).upper()

        # Priority 3: Standard dotted option format (X.)
        dot_matches = list(re.finditer(self.SIMPLE_DOT_PATTERN, answer, re.IGNORECASE))
        if dot_matches:
            return dot_matches[-1].group(1).upper()

        # Priority 4: Fallback to isolated character matching
        simple_matches = list(re.finditer(self.SIMPLE_PATTERN, answer, re.IGNORECASE))
        if simple_matches:
            return simple_matches[-1].group(1).upper()

        # Fallback: Return the upper case of the original string (likely yields 0 reward)
        return answer.upper()

    def __call__(self, completions: Union[str, List[str]], solution: Union[str, List[str]], **kwargs) -> List[float]:
        """
        Evaluates the consistency between the VLM's predictions and the ground truth.

        Args:
            completions (Union[str, List[str]]): Model-generated reasoning traces/answers.
            solution (Union[str, List[str]]): Ground truth options.
            **kwargs: Additional parameters passed by the Swift RL pipeline.

        Returns:
            List[float]: A list of scalar rewards (1.0 for correct, 0.0 for incorrect).
        """
        rewards = []

        # Standardize inputs to list format for batch processing
        if not isinstance(completions, list):
            completions = [completions]

        if not isinstance(solution, list):
            solution = [solution] * len(completions)

        for content, sol in zip(completions, solution):
            try:
                normalized_content = self.normalize_answer(content)
                normalized_solution = self.normalize_answer(sol)

                # Binary reward signal: 1.0 for strict equality, 0.0 otherwise
                reward = float(normalized_content == normalized_solution)

            except Exception as e:
                print(f"⚠️ [Reward Computation Error] Failed to process answer: {e}")
                reward = 0.0  # Default to zero reward on parsing failure

            rewards.append(reward)

        return rewards

# Register the reward function globally into the Swift ORM registry
orms['choice_accuracy'] = ChoiceAccuracy