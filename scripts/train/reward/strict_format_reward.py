# -*- coding: utf-8 -*-
"""
Strict Format Reward Function for GRPO Alignment
This module defines a rule-based reward function to enforce strict adherence to 
Chain-of-Thought (CoT) XML formatting constraints during RL training.
"""

import re
import logging
from typing import List
from swift.plugin import ORM, orms

log = logging.getLogger(__name__)

class StrictFormatReward(ORM):
    """
    Evaluates whether the model's generated response strictly contains 
    complete <think> and <answer> XML tags in the correct sequential order.

    This structural constraint is critical during the Logic-V alignment phase 
    to prevent "Cognitive Mimicry Collapse", ensuring the agent explicitly 
    externalizes its spatial and temporal reasoning process before concluding.
    """

    def __init__(self):
        super().__init__()
        # Pre-compile the regex for performance optimization over thousands of training steps.
        # re.DOTALL allows matching across newlines; re.IGNORECASE handles varying tag cases.
        self.format_pattern = re.compile(
            r"<think>.*?</think>.*?<answer>.*?</answer>",
            re.DOTALL | re.IGNORECASE
        )
        log.info("Initialized StrictFormatReward plugin (Strict Mode Enforced).")

    def __call__(self, completions: List[str], **kwargs) -> List[float]:
        """
        Calculates the format compliance reward for a batch of model completions.
        
        Args:
            completions (List[str]): The batch of text completions generated by the model.
            
        Returns:
            List[float]: A list of reward scores (1.0 for valid format, 0.0 for invalid).
        """
        rewards = []
        try:
            for completion in completions:
                if not isinstance(completion, str):
                    rewards.append(0.0)
                    continue
                
                # Search for the presence of the complete and sequential tag pair
                if self.format_pattern.search(completion):
                    rewards.append(1.0)
                else:
                    # Log a sanitized snippet of the malformed completion for debugging.
                    # Using log.debug to prevent console spam during early training epochs.
                    snippet = completion[:100].replace('\n', ' ')
                    log.debug(f"[StrictFormatReward] Malformed or truncated completion: {snippet}...")
                    rewards.append(0.0)
            
            return rewards
        
        except Exception as e:
            log.error(f"[StrictFormatReward] Critical error during evaluation: {e}", exc_info=True)
            # Fail-safe: penalize all if evaluation crashes to prevent corrupted gradients
            return [0.0] * len(completions)

# Register the plugin with the ms-swift ORM registry
orms['strict_format'] = StrictFormatReward